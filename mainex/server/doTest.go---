package server

import (
	"compress/gzip"
	. "db"
	"encoding/json"
	"fmt"
	"github.com/PuerkitoBio/goquery"
	"io/ioutil"
	"log"
	"mainex/lib/change"
	"mainex/lib/public"
	"mainex/structPack"
	"net/http"
	"net/url"
	"os"
	"strings"
)

func main() {

	listUrl(2)
}

//读取列表信息
func listUrl(page int) {
	startPage := cfg.Spage
	page = page + startPage
	url := cfg.Url
	url = fmt.Sprintf(url, page)

	html := GetHtml(url)

	//save(html, "28.html")
	Parsedocument(string(html), page)
}

//保存在文件中save(html, "28.html")
func save(body []byte, fileName string) {
	f, err := os.Create(fileName)
	if err != nil {
		panic(err)
	}
	//写入图信息
	_, err = f.Write(body)

	defer f.Close()
}

/*解析文档结构树*/
func Parsedocument(html string, page int) {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	//doc, err := goquery.NewDocument(url)
	if err != nil {
		log.Fatal(err)
		fmt.Println("解析文档错误")
	}

	ListP := cfg.List_dom
	ListA := cfg.List_url_dom
	doc.Find(ListP).Each(func(i int, s *goquery.Selection) {
		ListAUrl, _ := s.Find(ListA).Attr("href")
		fmt.Println("获取列表成功", ListAUrl)
		ViewInfo(ListAUrl, page)
	})
}

func GetHtml(url string) []byte {
	proxy := "http://104.224.15.64:8080/"
	resp, _ := GetByProxy(url, proxy)
	fmt.Printf("爬虫正在爬取链接[%v],爬取状态:%v\n", url, resp.StatusCode)
	defer resp.Body.Close()

	if resp.StatusCode == 200 {
		var body []byte

		switch resp.Header.Get("Content-Encoding") {
		case "gzip":
			reader, _ := gzip.NewReader(resp.Body)
			body, _ = ioutil.ReadAll(reader) //dumpGZIP
		default:
			bodyByte, _ := ioutil.ReadAll(resp.Body)
			body = bodyByte
		}
		//fmt.Println(string(body))
		return body
	}
	//body, _ := ioutil.ReadAll(resp.Body)
	return nil
}

// http get by proxy 模仿ip代理
func GetByProxy(url_addr, proxy_addr string) (*http.Response, error) {
	url, _ := url.Parse(url_addr)
	request, _ := http.NewRequest("GET", url.String(), nil)

	proxy, err := url.Parse(proxy_addr)
	if err != nil {
		return nil, err
	}

	headerSet := new(structPack.HeaderSet)
	//cfg.HeaderSet
	map_ := change.Struct.ToMapAddr(headerSet)
	for k, v := range map_ {
		if v != "" {
			request.Header.Set(k, v.(string))
		}
	}

	client := &http.Client{
		Transport: &http.Transport{
		//Proxy: http.ProxyURL(proxy), //ip代理已注释掉
		},
	}
	fmt.Println("代理地址未使用:", proxy)
	return client.Do(request)
}

//解析列表
func ViewInfo(Url string, pages int) error {
	ViewHost := cfg.Domain
	var ViewUrl = ""
	if ViewHost != "" {
		ViewUrl = ViewHost + Url
	} else {
		ViewUrl = Url
	}
	if bloom.Check([]byte(ViewUrl)) {
		fmt.Println("该链接已爬取过" + ViewUrl)
		return nil
	} else {
		bloom.Add([]byte(ViewUrl))
	}

	doc, err := goquery.NewDocument(ViewUrl)
	if err != nil {
		log.Fatal(err)
	}

	tmpArticle := &structPack.Anote{}

	//解析文章
	tmpArticle.Content, err = doc.Find(cfg.View_content_dom).Html()
	if err != nil {
		log.Fatal("文章内容解析失败")
	}
	tmpArticle.Title = doc.Find(cfg.View_title_dom).Text()
	tmpArticle.Is_auto = 1
	tmpArticle.Is_open = 1
	tmpArticle.Cateid = 31
	tmpArticle.Is_auto_page = pages
	tmpArticle.Is_auto_source = cfg.Name
	tmpArticle.Url = ViewUrl
	//save([]byte(tmpArticle.Content), tmpArticle.Title+".html")
	//封面图
	cover := public.Publics.CoverGirl(tmpArticle.Content, 3) //[]string
	lang, _ := json.Marshal(cover)                           //转json str
	var scover string
	if len(cover) != 0 {
		scover = string(lang)
	} else {
		scover = ""
	}
	tmpArticle.Cover = scover
	tmpArticle.Newslist_tpl = len(cover)

	//解析文章end
	fmt.Printf("get content=====%v\n", tmpArticle)

	return nil

}
